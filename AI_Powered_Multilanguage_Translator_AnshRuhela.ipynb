{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<u><h1><b>AI-Powered Multi-Language Translator</b></h1></u>\n",
        "This project involves building a generative AI-powered translator that enables natural and context-aware text translation across multiple languages. We will explore NLP concepts, prompt engineering, and large language model integration for seamless multilingual communication.\n",
        "<hr>\n",
        "The application is built using:<br>\n",
        "1. Transformers library (for using pre-trained translation models)<br>\n",
        "2. Gradio (for creating a simple and interactive web interface)<br>\n",
        "3. Python / Google Colab (for development and deployment)<br>\n",
        "\n",
        "<pre>\n",
        "Model: <u>facebook/m2m100_418M</u>\n",
        "</pre>\n",
        "\n",
        "<pre>\n",
        "supported_languages = {\n",
        "    \"en\": \"English\",\n",
        "    \"fr\": \"French\",\n",
        "    \"de\": \"German\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    # and more...\n",
        "}\n",
        "</pre>"
      ],
      "metadata": {
        "id": "W7-iIBV86IkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 :- <b>Install Required Libraries</b><br>"
      ],
      "metadata": {
        "id": "Eb8T-SdVXnSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers sentencepiece langdetect"
      ],
      "metadata": {
        "id": "VJjvecEb6Tvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7008fa5-513c-4b13-817f-6167185a5925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 :- <b>Import Dependencies</b>"
      ],
      "metadata": {
        "id": "0fyhFDnD6YcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByUFcg4GxY3-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from langdetect import detect\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 :- Configure Huggingface <b>Access Token</b> and <i>Supported Languages</i> for <i>Translation</i>"
      ],
      "metadata": {
        "id": "xRy1JzyvYc7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "# List of supported languages\n",
        "supported_languages = [\n",
        "    'en',  # English\n",
        "    'fr',  # French\n",
        "    'de',  # German\n",
        "    'es',  # Spanish\n",
        "    'it',  # Italian\n",
        "    'ru',  # Russian\n",
        "    'zh',  # Chinese\n",
        "    'ar',  # Arabic\n",
        "    'hi',  # Hindi\n",
        "    'bn',  # Bengali\n",
        "    'ja',  # Japanese\n",
        "    'ko',  # Korean\n",
        "    'pt',  # Portuguese\n",
        "    'tr',  # Turkish\n",
        "    'vi',  # Vietnamese\n",
        "]\n",
        "\n",
        "# Map language code to readable name for user\n",
        "lang_names = {\n",
        "    'en': 'English', 'fr': 'French', 'de': 'German', 'es': 'Spanish', 'it': 'Italian',\n",
        "    'ru': 'Russian', 'zh': 'Chinese', 'ar': 'Arabic', 'hi': 'Hindi', 'bn': 'Bengali',\n",
        "    'ja': 'Japanese', 'ko': 'Korean', 'pt': 'Portuguese', 'tr': 'Turkish', 'vi': 'Vietnamese'\n",
        "}\n"
      ],
      "metadata": {
        "id": "4cnrxEtr19S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 :- <b>Download Model from Huggingface</b>"
      ],
      "metadata": {
        "id": "cse9vXXKZC1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "MODEL_NAME = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "print(f\"‚úÖ Model & tokenizer `{MODEL_NAME}` loaded.\")"
      ],
      "metadata": {
        "id": "FPiZzsDdZLwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "084dc515-e4ef-4557-8b01-b6ebf640b961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model & tokenizer `facebook/m2m100_418M` loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 :- <b>Load Model to GPU for <i>faster Inference</i></b>"
      ],
      "metadata": {
        "id": "lSrYuzeoZOVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use CUDA if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "h-L91lLeZYPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2133e29-0c7b-495e-f52a-3bc737fb901e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 :- Accept User Input Language Code"
      ],
      "metadata": {
        "id": "JKa25z8hZ6kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For translation, set source language.\n",
        "print(\"Choose source language code:\")\n",
        "for code, name in lang_names.items():\n",
        "    print(f\"{code}: {name}\")\n",
        "src_lang = input(\"Enter your source language code: \").strip()\n",
        "tokenizer.src_lang = src_lang"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqpWTOFlZ-DO",
        "outputId": "037a8039-465a-40ce-cf82-b22c9a8f6cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose source language code:\n",
            "en: English\n",
            "fr: French\n",
            "de: German\n",
            "es: Spanish\n",
            "it: Italian\n",
            "ru: Russian\n",
            "zh: Chinese\n",
            "ar: Arabic\n",
            "hi: Hindi\n",
            "bn: Bengali\n",
            "ja: Japanese\n",
            "ko: Korean\n",
            "pt: Portuguese\n",
            "tr: Turkish\n",
            "vi: Vietnamese\n",
            "Enter your source language code: en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7 :- Accept User's Input"
      ],
      "metadata": {
        "id": "fbCys9JIawNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "input_text = input(\"Enter text: \").strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69LWWdVxa1D5",
        "outputId": "d0a6ce20-6a69-40e3-e071-429b47822c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter text: hello my name is ansh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8 :- Accept Output Language Code"
      ],
      "metadata": {
        "id": "_E4X7dYNaAZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For translation, set target language.\n",
        "print(\"Choose target language code:\")\n",
        "for code, name in lang_names.items():\n",
        "    print(f\"{code}: {name}\")\n",
        "tgt_lang = input(\"Enter language code from above: \").strip()\n",
        "\n",
        "if tgt_lang not in supported_languages:\n",
        "    print(\"Language not supported.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSNsBuKqaEs3",
        "outputId": "5fc5e542-4553-4af2-9ada-c8e5f5238d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose target language code:\n",
            "en: English\n",
            "fr: French\n",
            "de: German\n",
            "es: Spanish\n",
            "it: Italian\n",
            "ru: Russian\n",
            "zh: Chinese\n",
            "ar: Arabic\n",
            "hi: Hindi\n",
            "bn: Bengali\n",
            "ja: Japanese\n",
            "ko: Korean\n",
            "pt: Portuguese\n",
            "tr: Turkish\n",
            "vi: Vietnamese\n",
            "Enter language code from above: hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "encoded"
      ],
      "metadata": {
        "id": "O47p9BAhbMCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "433ae1bd-fdd5-41ed-ff18-0a95b23121de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[128022, 110013,   1949,  33969,    117,     48,   1537,      2]],\n",
              "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_tokens = model.generate(\n",
        "    **encoded,\n",
        "    forced_bos_token_id=tokenizer.get_lang_id(tgt_lang)\n",
        "    )\n",
        "generated_tokens"
      ],
      "metadata": {
        "id": "UxgDQXENbPL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ff7791-1e2c-460d-fb8e-b042cf3cd86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[     2, 128036,    776,  10484,  57545,  15392,   5220,   3844,   3188,\n",
              "            776,      2]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translation = tokenizer.batch_decode(\n",
        "  generated_tokens, skip_special_tokens=True)[0]\n",
        "translation"
      ],
      "metadata": {
        "id": "7x5ytX-_2TSs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f41e2ae0-411b-413e-cd4c-02a0101173a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'‡§π‡•à‡§≤‡•ã ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§è‡§®‡•ç‡§∂ ‡§π‡•à'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Translation ({lang_names.get(tgt_lang, tgt_lang)}):\", translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS4YuUfibT4M",
        "outputId": "b1db2172-803a-443c-f28d-4bc8f0420820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation (Hindi): ‡§π‡•à‡§≤‡•ã ‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§è‡§®‡•ç‡§∂ ‡§π‡•à\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Integrating all logic into seperate function</h2>"
      ],
      "metadata": {
        "id": "oiuox9TdkR6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from_choices = [(\"Auto-detect\", \"auto\")] + [\n",
        "    (lang_names[code], code) for code in supported_languages\n",
        "]\n",
        "to_choices = [(lang_names[code], code) for code in supported_languages]\n",
        "\n",
        "def translate(text, from_lang, to_lang):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # 1) auto-detect if needed\n",
        "    if from_lang == \"auto\":\n",
        "        guessed = detect(text)\n",
        "        if guessed in supported_languages:\n",
        "            from_lang = guessed\n",
        "    # 2) encode & generate\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=tokenizer.get_lang_id(to_lang)\n",
        "    )\n",
        "    # 3) decode\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "tSHjBGfvkRj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Preparing Gradio UI</h2>"
      ],
      "metadata": {
        "id": "2sEp6118S46J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the interface\n",
        "iface = gr.Interface(\n",
        "    fn=translate,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=3, label=\"Input Text\", placeholder=\"Type here‚Ä¶\"),\n",
        "        gr.Dropdown(choices=from_choices, label=\"From\", value=\"auto\"),\n",
        "        gr.Dropdown(choices=to_choices,   label=\"To\",   value=\"en\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Translation\"),\n",
        "    title=\"üåê Translator \",\n",
        "    description=\"Auto-detect or pick one input language, then choose your target language. Transaltion result will be shown in output.\",\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "9Ji4d9FJdGEI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "1ddbc050-e2c1-4b35-e3ca-09abf12c6d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f0179d6def478341f4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f0179d6def478341f4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTkbPW6-PR9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}